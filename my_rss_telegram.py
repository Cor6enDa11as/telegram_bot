#!/usr/bin/env python3
import feedparser
import time
import requests
import re
import html
from datetime import datetime
import os
from flask import Flask
import threading
from urllib.parse import urlparse
from bs4 import BeautifulSoup

app = Flask(__name__)

TELEGRAM_BOT_TOKEN = os.getenv('TELEGRAM_BOT_TOKEN')
TELEGRAM_CHANNEL_ID = os.getenv('TELEGRAM_CHANNEL_ID')

if not TELEGRAM_BOT_TOKEN or not TELEGRAM_CHANNEL_ID:
    print("‚ùå –û—à–∏–±–∫–∞: –ù–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã TELEGRAM_BOT_TOKEN –∏–ª–∏ TELEGRAM_CHANNEL_ID")
    exit(1)

RSS_SOURCES = [
    {"url": "https://habr.com/ru/rss/hubs/linux_dev/articles/?fl=ru", "hashtag": "#linux"},
    {"url": "https://habr.com/ru/rss/hubs/linux/articles/?fl=ru", "hashtag": "#linux"},
    {"url": "https://habr.com/ru/rss/hubs/popular_science/articles/?fl=ru", "hashtag": "#–Ω–∞—É–∫–∞"},
    {"url": "https://habr.com/ru/rss/hubs/astronomy/articles/?fl=ru", "hashtag": "#–∞—Å—Ç—Ä–æ–Ω–æ–º–∏—è"},
    {"url": "https://habr.com/ru/rss/flows/popsci/articles/?fl=ru", "hashtag": "#–Ω–∞—É–∫–∞"},
    {"url": "https://4pda.to/feed/", "hashtag": "#–º–æ–±–∏–ª—å–Ω—ã–µ"},
    {"url": "https://tech.onliner.by/feed", "hashtag": "#—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏"},
    {"url": "https://www.ixbt.com/export/news.rss", "hashtag": "#—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏"},
    {"url": "https://androidinsider.ru/feed", "hashtag": "#android"},
    {"url": "https://naked-science.ru/feed", "hashtag": "#–Ω–∞—É–∫–∞"},
    {"url": "https://www.opennet.ru/opennews/opennews_full_utf.rss", "hashtag": "#linux"},
    {"url": "https://www.comss.ru/linux.php", "hashtag": "#linux"},
    {"url": "https://www.linux.org.ru/section-rss.jsp?section=1", "hashtag": "#linux"},
    {"url": "https://www.phoronix.com/rss.php", "hashtag": "#linux"},
    {"url": "https://linuxiac.com/feed/", "hashtag": "#linux"},
    {"url": "https://www.linuxinsider.com/rss-feed", "hashtag": "#linux"},
    {"url": "https://distrowatch.com/news/dw.xml", "hashtag": "#linux"},
    {"url": "https://9to5linux.com/feed/", "hashtag": "#linux"},
    {"url": "https://www.gamingonlinux.com/article_rss.php", "hashtag": "#linux"},
    {"url": "https://itsfoss.com/feed/", "hashtag": "#linux"},
    {"url": "https://www.omgubuntu.co.uk/feed/", "hashtag": "#linux"},
    {"url": "https://rozetked.me/rss.xml", "hashtag": "#—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏"},
    {"url": "https://mobile-review.com/all/news/feed/", "hashtag": "#android"},
    {"url": "https://droider.ru/feed", "hashtag": "#—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏"},
    {"url": "https://www.comss.ru/linux.php", "hashtag": "#linux"},
    {"url": "https://rss-bridge.org/bridge01/?action=display&bridge=YouTubeFeedExpanderBridge&channel=UCt75WMud0RMUivGBNzvBPXQ&embed=on&format=Mrss" , "hashtag": "#–ü–æ–ª—å–∑–∞ NET"},
    {"url": "https://rss-bridge.org/bridge01/?action=display&bridge=TelegramBridge&username=%40prohitec&format=Mrss" , "hashtag": "#PRO Hi-Tech"},
]

def parse_feed(url):
    """–ü–∞—Ä—Å–∏—Ç RSS —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        'Accept': 'application/rss+xml, application/xml, text/xml'
    }

    try:
        response = requests.get(url, headers=headers, timeout=10)
        content = response.content

        if any(site in url for site in ['4pda.to', 'ixbt.com']):
            try:
                content = content.decode('windows-1251').encode('utf-8')
            except:
                pass

        feed = feedparser.parse(content)

        if feed.bozo and feed.entries:
            print(f"   ‚ö†Ô∏è –ï—Å—Ç—å –æ—à–∏–±–∫–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞, –Ω–æ –Ω–æ–≤–æ—Å—Ç–∏ –Ω–∞–π–¥–µ–Ω—ã: {feed.bozo_exception}")
            return feed
        elif feed.entries:
            return feed
        else:
            print(f"   ‚ùå –ù–µ—Ç –Ω–æ–≤–æ—Å—Ç–µ–π –≤ —Ñ–∏–¥–µ")
            return feedparser.parse("")

    except Exception as e:
        print(f"üí• –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ {url}: {e}")
        return feedparser.parse("")

def is_russian_text(text):
    if not text:
        return False
    cyrillic_count = sum(1 for char in text if '\u0400' <= char <= '\u04FF')
    total_letters = sum(1 for char in text if char.isalpha())
    return (cyrillic_count / total_letters) > 0.3 if total_letters >= 3 else False

def translate_text(text):
    try:
        if not text or not text.strip():
            return text
        url = "https://translate.googleapis.com/translate_a/single"
        params = {'client': 'gtx', 'sl': 'auto', 'tl': 'ru', 'dt': 't', 'q': text}
        response = requests.get(url, params=params, timeout=10)
        return response.json()[0][0][0] if response.status_code == 200 else text
    except Exception:
        return text

def prepare_news_content(title, description):
    was_translated = False
    processed_title = title

    if not is_russian_text(title):
        translated_title = translate_text(title)
        if translated_title and translated_title != title:
            processed_title = translated_title
            was_translated = True

    processed_description = ""
    if description:
        clean_desc = re.sub('<[^<]+?>', '', description)
        clean_desc = html.unescape(clean_desc)
        clean_desc = re.sub(r'\s+', ' ', clean_desc).strip()
        clean_desc = re.sub(r'<[^>]*$', '', clean_desc)
        clean_desc = re.sub(r'^[^<]*>', '', clean_desc)

        if len(clean_desc) > 400:
            clean_desc = clean_desc[:400] + "..."
        if not is_russian_text(clean_desc) and clean_desc.strip():
            translated_desc = translate_text(clean_desc)
            if translated_desc and translated_desc != clean_desc:
                processed_description = translated_desc
                was_translated = True
            else:
                processed_description = clean_desc
        else:
            processed_description = clean_desc

    return processed_title, processed_description, was_translated

def extract_image_from_entry(entry):
    """–ü–æ–∏—Å–∫ –∫–∞—Ä—Ç–∏–Ω–æ–∫ –≤ RSS –∑–∞–ø–∏—Å–∏"""
    try:
        if hasattr(entry, 'links'):
            for link in entry.links:
                if 'image' in link.type:
                    return link.href
                if hasattr(link, 'rel') and 'enclosure' in link.rel:
                    if 'image' in getattr(link, 'type', ''):
                        return link.href

        if hasattr(entry, 'media_content'):
            for media in entry.media_content:
                if media.get('type', '').startswith('image/'):
                    return media['url']

        if hasattr(entry, 'media_thumbnail') and entry.media_thumbnail:
            return entry.media_thumbnail[0]['url']

        content_fields = ['summary', 'content', 'description', 'content_encoded']
        for field in content_fields:
            if hasattr(entry, field):
                content = getattr(entry, field)
                if isinstance(content, list):
                    content = content[0].value if content else ""
                if content:
                    img_match = re.search(r'<img[^>]+src="([^">]+)"', content)
                    if img_match:
                        img_url = img_match.group(1)
                        if any(ext in img_url.lower() for ext in ['.jpg', '.jpeg', '.png', '.gif', '.webp']):
                            return img_url

        if hasattr(entry, 'enclosures'):
            for enclosure in entry.enclosures:
                if 'image' in getattr(enclosure, 'type', ''):
                    return enclosure.href

    except Exception as e:
        print(f"üí• –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –∫–∞—Ä—Ç–∏–Ω–∫–∏ –≤ RSS: {e}")

    return None

def extract_image_from_page(link):
    """–ü–∞—Ä—Å–∏—Ç –≥–ª–∞–≤–Ω—É—é –∫–∞—Ä—Ç–∏–Ω–∫—É —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –Ω–æ–≤–æ—Å—Ç–∏"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml'
        }

        response = requests.get(link, headers=headers, timeout=10)
        soup = BeautifulSoup(response.content, 'html.parser')

        # –°–Ω–∞—á–∞–ª–∞ –∏—â–µ–º –≤ meta-—Ç–µ–≥–∞—Ö (—Å–∞–º—ã–µ –Ω–∞–¥–µ–∂–Ω—ã–µ)
        meta_selectors = [
            'meta[property="og:image"]',
            'meta[name="twitter:image"]',
            'meta[property="og:image:url"]',
            'link[rel="image_src"]'
        ]

        for selector in meta_selectors:
            meta_tag = soup.select_one(selector)
            if meta_tag:
                image_url = meta_tag.get('content') or meta_tag.get('href')
                if image_url and image_url.startswith('http'):
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —ç—Ç–æ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –∫–∞—Ä—Ç–∏–Ω–∫–∞
                    if any(ext in image_url.lower() for ext in ['.jpg', '.jpeg', '.png', '.gif', '.webp']):
                        print(f"   üñºÔ∏è –ù–∞–π–¥–µ–Ω–∞ –∫–∞—Ä—Ç–∏–Ω–∫–∞ –≤ meta-—Ç–µ–≥–∞—Ö")
                        return image_url

        # –ò—â–µ–º –≥–ª–∞–≤–Ω—É—é –∫–∞—Ä—Ç–∏–Ω–∫—É –≤ –∫–æ–Ω—Ç–µ–Ω—Ç–µ
        content_selectors = [
            'article img:first-of-type',
            '.content img:first-of-type',
            'main img:first-of-type',
            '.post-content img:first-of-type',
            '.article img:first-of-type',
            'img[class*="hero"]',
            'img[class*="main"]',
            'img[class*="featured"]',
            'img[class*="cover"]'
        ]

        for selector in content_selectors:
            img_tag = soup.select_one(selector)
            if img_tag:
                image_url = img_tag.get('src')
                if image_url and image_url.startswith('http'):
                    if any(ext in image_url.lower() for ext in ['.jpg', '.jpeg', '.png', '.gif', '.webp']):
                        print(f"   üñºÔ∏è –ù–∞–π–¥–µ–Ω–∞ –∫–∞—Ä—Ç–∏–Ω–∫–∞ –≤ –∫–æ–Ω—Ç–µ–Ω—Ç–µ: {selector}")
                        return image_url
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º data-src –¥–ª—è –ª–µ–Ω–∏–≤–æ–π –∑–∞–≥—Ä—É–∑–∫–∏
                elif img_tag.get('data-src'):
                    image_url = img_tag.get('data-src')
                    if image_url.startswith('http'):
                        if any(ext in image_url.lower() for ext in ['.jpg', '.jpeg', '.png', '.gif', '.webp']):
                            print(f"   üñºÔ∏è –ù–∞–π–¥–µ–Ω–∞ –∫–∞—Ä—Ç–∏–Ω–∫–∞ –≤ data-src: {selector}")
                            return image_url

        print(f"   ‚ùå –ö–∞—Ä—Ç–∏–Ω–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ")
        return None

    except Exception as e:
        print(f"üí• –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {link}: {e}")
        return None

def get_news_image(entry, link):
    """–ü—ã—Ç–∞–µ—Ç—Å—è –ø–æ–ª—É—á–∏—Ç—å –∫–∞—Ä—Ç–∏–Ω–∫—É —Ä–∞–∑–Ω—ã–º–∏ —Å–ø–æ—Å–æ–±–∞–º–∏"""
    # 1. –°–Ω–∞—á–∞–ª–∞ –∏–∑ RSS (–±—ã—Å—Ç—Ä–µ–µ)
    image_url = extract_image_from_entry(entry)
    if image_url:
        print(f"   üñºÔ∏è –ö–∞—Ä—Ç–∏–Ω–∫–∞ –Ω–∞–π–¥–µ–Ω–∞ –≤ RSS")
        return image_url

    # 2. –ï—Å–ª–∏ –≤ RSS –Ω–µ—Ç - –ø–∞—Ä—Å–∏–º —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
    print(f"   üîç –ö–∞—Ä—Ç–∏–Ω–∫–∏ –≤ RSS –Ω–µ—Ç, –ø–∞—Ä—Å–∏–º —Å—Ç—Ä–∞–Ω–∏—Ü—É...")
    image_url = extract_image_from_page(link)
    if image_url:
        print(f"   üñºÔ∏è –ö–∞—Ä—Ç–∏–Ω–∫–∞ –Ω–∞–π–¥–µ–Ω–∞ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ")
        return image_url

    # 3. –ï—Å–ª–∏ –≤—Å—ë —Ä–∞–≤–Ω–æ –Ω–µ—Ç - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º None
    print(f"   ‚ùå –ö–∞—Ä—Ç–∏–Ω–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –Ω–∏ –≤ RSS, –Ω–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ")
    return None

def create_news_message(domain, title, description, link, pub_date, was_translated, hashtag):
    """–°–æ–∑–¥–∞–µ—Ç –∫—Ä–∞—Å–∏–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ —Å markdown —Ä–∞–∑–º–µ—Ç–∫–æ–π"""
    message_parts = [
        f"üåê  {domain}",
        "",
        f"‚ö°  *{title}*",
    ]

    if description:
        message_parts.append("")
        message_parts.append(f"‚ú®  _{description}_")

    message_parts.extend([
        "",
        f"üîó  [–ß–∏—Ç–∞—Ç—å]({link})",
        "",
        f"üìÖ  {pub_date}",
    ])

    if hashtag:
        message_parts.append("")
        message_parts.append(f"üè∑Ô∏è {hashtag}")

    if was_translated:
        message_parts.append("")
        message_parts.append("`üî§ [–ü–µ—Ä–µ–≤–µ–¥–µ–Ω–æ]`")

    return "\n".join(message_parts)

def send_news_message(title, description, link, pub_date, image_url=None, was_translated=False, hashtag=""):
    """–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏–µ –≤ Telegram"""
    try:
        domain = urlparse(link).netloc.replace('www.', '')
        message_text = create_news_message(domain, title, description, link, pub_date, was_translated, hashtag)

        # –í—Å–µ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º –Ω–∞—à —Ñ–æ—Ä–º–∞—Ç —Å –∫–∞—Ä—Ç–∏–Ω–∫–æ–π –µ—Å–ª–∏ –µ—Å—Ç—å
        if image_url:
            # –ü—Ä–æ–±—É–µ–º –æ—Ç–ø—Ä–∞–≤–∏—Ç—å —Å –∫–∞—Ä—Ç–∏–Ω–∫–æ–π
            url = f"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/sendPhoto"
            data = {
                'chat_id': TELEGRAM_CHANNEL_ID,
                'photo': image_url,
                'caption': message_text,
                'parse_mode': 'Markdown'
            }

            response = requests.post(url, data=data, timeout=10)

            # –ï—Å–ª–∏ –Ω–µ –ø–æ–ª—É—á–∏–ª–æ—Å—å —Å –∫–∞—Ä—Ç–∏–Ω–∫–æ–π, –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º —Ç–µ–∫—Å—Ç–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
            if response.status_code != 200:
                print(f"   ‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–ø—Ä–∞–≤–∏—Ç—å —Å –∫–∞—Ä—Ç–∏–Ω–∫–æ–π, –ø—Ä–æ–±—É—é —Ç–µ–∫—Å—Ç–æ–≤–æ–µ")
                url = f"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/sendMessage"
                data = {
                    'chat_id': TELEGRAM_CHANNEL_ID,
                    'text': message_text,
                    'parse_mode': 'Markdown',
                    'disable_web_page_preview': True
                }
                response = requests.post(url, data=data, timeout=10)
        else:
            # –ë–µ–∑ –∫–∞—Ä—Ç–∏–Ω–∫–∏ - —Ç–µ–∫—Å—Ç–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
            url = f"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/sendMessage"
            data = {
                'chat_id': TELEGRAM_CHANNEL_ID,
                'text': message_text,
                'parse_mode': 'Markdown',
                'disable_web_page_preview': True
            }
            response = requests.post(url, data=data, timeout=10)

        if response.status_code == 200:
            print(f"‚úÖ –û—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ: {title[:50]}... {hashtag}")
            return True
        else:
            print(f"‚ùå –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏: {response.status_code}")
            return False

    except Exception as e:
        print(f"üí• –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏: {e}")
        return False

def run_bot():
    last_links = {}
    print("üöÄ –ë–æ—Ç –∑–∞–ø—É—â–µ–Ω...")

    for source in RSS_SOURCES:
        url, hashtag = source["url"], source["hashtag"]
        try:
            feed = parse_feed(url)
            if feed.entries:
                last_links[url] = feed.entries[0].link
                print(f"‚úÖ {urlparse(url).netloc} {hashtag}")
            elif feed.bozo:
                print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ {url}: {feed.bozo_exception}")
        except Exception as e:
            print(f"üí• –û—à–∏–±–∫–∞ {url}: {e}")

    while True:
        try:
            print(f"\nüîç –ü—Ä–æ–≤–µ—Ä–∫–∞... ({datetime.now().strftime('%H:%M:%S')})")
            found_new_news = False

            for source in RSS_SOURCES:
                url, hashtag = source["url"], source["hashtag"]

                try:
                    feed = parse_feed(url)
                    if not feed.entries:
                        continue

                    latest, link = feed.entries[0], feed.entries[0].link

                    if url not in last_links or last_links[url] != link:
                        if url in last_links:
                            print(f"üéâ –ù–û–í–û–°–¢–¨: {hashtag}")
                            found_new_news = True

                        pub_date = "–î–∞—Ç–∞ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–∞"
                        if hasattr(latest, 'published_parsed') and latest.published_parsed:
                            pub_date = datetime(*latest.published_parsed[:6]).strftime("%d.%m.%Y %H:%M")

                        title, description, was_translated = prepare_news_content(
                            latest.title,
                            latest.description if hasattr(latest, 'description') else ""
                        )

                        # –£–õ–£–ß–®–ï–ù–ù–´–ô –ü–û–ò–°–ö –ö–ê–†–¢–ò–ù–ö–ò: —Å–Ω–∞—á–∞–ª–∞ RSS, –ø–æ—Ç–æ–º —Å—Ç—Ä–∞–Ω–∏—Ü–∞
                        image_url = get_news_image(latest, link)

                        if send_news_message(title, description, link, pub_date, image_url, was_translated, hashtag):
                            last_links[url] = link

                except Exception as e:
                    print(f"üí• –û—à–∏–±–∫–∞ {url}: {e}")

            print(f"‚è∞ –û–∂–∏–¥–∞–Ω–∏–µ 15 –º–∏–Ω—É—Ç..." if not found_new_news else "‚úÖ –ù–æ–≤–æ—Å—Ç–∏ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω—ã")
            time.sleep(900)

        except Exception as e:
            print(f"üí• –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
            time.sleep(60)

@app.route('/')
def home():
    return "ü§ñ Telegram RSS Bot —Ä–∞–±–æ—Ç–∞–µ—Ç!"

@app.route('/ping')
def ping():
    return "pong"

bot_thread = threading.Thread(target=run_bot)
bot_thread.daemon = True
bot_thread.start()

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000, debug=False)
