#!/usr/bin/env python3
"""
üöÄ RSS to Telegram Bot (Termux Optimized)
"""

import os
import json
import feedparser
import requests
import time
import logging
import random
import re
from datetime import datetime, timezone, timedelta
#from dotenv import load_dotenv
from urllib.parse import urlparse

# ==================== –ó–∞–≥—Ä—É–∑–∫–∞ –Ω–∞—Å—Ç—Ä–æ–µ–∫ ====================
#load_dotenv()
#BOT_TOKEN = os.getenv('BOT_TOKEN')
#CHANNEL_ID = os.getenv('CHANNEL_ID')

#if not BOT_TOKEN or not CHANNEL_ID:
#    logging.error("‚ùå –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ BOT_TOKEN –∏ CHANNEL_ID –≤ .env —Ñ–∞–π–ª–µ!")
#    exit(1)

# ‚úÖ –¢–ï–†–ú–ò–ù–ê–õ–¨–ù–´–ï –ù–ê–°–¢–†–û–ô–ö–ò TERMUX
CONFIG = {
    'REQUEST_DELAY_MIN': int(os.getenv('REQUEST_DELAY_MIN', '8')),
    'REQUEST_DELAY_MAX': int(os.getenv('REQUEST_DELAY_MAX', '20')),
    'MAX_HOURS_BACK': int(os.getenv('MAX_HOURS_BACK', '4'))
}

# ==================== –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è ====================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[logging.FileHandler('bot.log'), logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

# ==================== –§—É–Ω–∫—Ü–∏–∏ ====================

def get_entry_image(entry):
    """üñºÔ∏è –ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–∞—Ä—Ç–∏–Ω–∫—É –∏–∑ RSS –∑–∞–ø–∏—Å–∏"""
    image_candidates = [
        getattr(entry, 'enclosures', [{}])[0].get('href') if entry.enclosures else None,
        getattr(entry, 'media_content', [{}])[0].get('url') if hasattr(entry, 'media_content') and entry.media_content else None,
        getattr(entry, 'media_thumbnail', [{}])[0].get('url') if hasattr(entry, 'media_thumbnail') and entry.media_thumbnail else None,
        getattr(entry, 'image', {}).get('href') if hasattr(entry, 'image') else None,
    ]

    for img_url in image_candidates:
        if img_url and (img_url.startswith('http') or img_url.startswith('//')):
            if img_url.startswith('//'):
                base_url = getattr(entry, 'base', '')
                if base_url.startswith('http'):
                    parsed_base = urlparse(base_url)
                    img_url = f"{parsed_base.scheme}:{img_url}"
                else:
                    continue
            return img_url
    return None

def clean_description(description):
    """üßπ –û—á–∏—â–∞–µ—Ç HTML —Ç–µ–≥–∏ –∏ —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª—ã"""
    if not description:
        return ''
    
    description = description.strip()
    description = re.sub(r'<[^>]+>', '', description)
    description = description.replace('&', '&amp;').replace('<', '&lt;').replace('>', '&gt;')
    
    if len(description) > 300:
        description = description[:300] + '...'
    
    return description

def format_publication_date(pub_date):
    """üìÖ –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –¥–∞—Ç—É –ø—É–±–ª–∏–∫–∞—Ü–∏–∏"""
    return pub_date.strftime('%d.%m.%Y %H:%M')

def send_to_telegram(title, link, feed_url, hashtags_dict, entry, pub_date):
    """üì® –û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏–µ —Å –Ω–æ–≤—ã–º —Ñ–æ—Ä–º–∞—Ç–æ–º"""
    try:
        clean_title = title.replace('&', '&amp;').replace('<', '&lt;').replace('>', '&gt;')
        hashtag = hashtags_dict.get(feed_url, '#–Ω–æ–≤–æ—Å—Ç–∏')
        author = getattr(entry, 'author', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∞–≤—Ç–æ—Ä').strip()
        
        original_description = getattr(entry, 'summary', '') or getattr(entry, 'description', '')
        description = clean_description(original_description)
        
        logger.info(f"  üìù –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏—è: {title[:50]}...")
        
        image_url = None
        
        if hasattr(entry, 'enclosures') and entry.enclosures:
            for enc in entry.enclosures:
                url = enc.get('href', '') or enc.get('url', '')
                if url and (url.endswith('.jpg') or url.endswith('.jpeg') or 
                           url.endswith('.png') or url.endswith('.gif') or
                           'image' in enc.get('type', '').lower()):
                    image_url = url
                    logger.info(f"  üñºÔ∏è –ù–∞–π–¥–µ–Ω–æ –≤ enclosures: {url[:60]}...")
                    break
        
        if not image_url and hasattr(entry, 'media_content'):
            for media in entry.media_content:
                url = media.get('url', '')
                if url and ('image' in media.get('type', '').lower() or
                           url.endswith(('.jpg', '.jpeg', '.png', '.gif', '.webp'))):
                    image_url = url
                    logger.info(f"  üñºÔ∏è –ù–∞–π–¥–µ–Ω–æ –≤ media_content: {url[:60]}...")
                    break
        
        if not image_url and hasattr(entry, 'media_thumbnail'):
            if entry.media_thumbnail:
                url = entry.media_thumbnail[0].get('url', '')
                if url:
                    image_url = url
                    logger.info(f"  üñºÔ∏è –ù–∞–π–¥–µ–Ω–æ –≤ media_thumbnail: {url[:60]}...")
        
        if not image_url and hasattr(entry, 'image'):
            url = entry.image.get('href', '') or entry.image.get('url', '')
            if url:
                image_url = url
                logger.info(f"  üñºÔ∏è –ù–∞–π–¥–µ–Ω–æ –≤ image: {url[:60]}...")
        
        if not image_url and original_description:
            import re
            img_patterns = [
                r'<img[^>]+src="([^">]+)"',
                r"<img[^>]+src='([^'>]+)'",
                r'<img[^>]+src=([^\s>]+)'
            ]
            
            for pattern in img_patterns:
                match = re.search(pattern, original_description, re.IGNORECASE)
                if match:
                    image_url = match.group(1)
                    logger.info(f"  üñºÔ∏è –ù–∞–π–¥–µ–Ω–æ –≤ HTML-–æ–ø–∏—Å–∞–Ω–∏–∏: {image_url[:60]}...")
                    break
        
        if not image_url:
            logger.info("  ‚ö†Ô∏è –ö–∞—Ä—Ç–∏–Ω–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ RSS")
        
        message_text = f'<a href="{link}">{clean_title}</a>'
        
        if description:
            message_text += f'\n\n<i>{description}</i>'
        
        author_hashtag = author.replace(" ", "")
        message_text += f'\n\nüìå  {hashtag} üë§  #{author_hashtag}'
        
        if image_url:
            try:
                if image_url.startswith('//'):
                    image_url = 'https:' + image_url
                    logger.info(f"  üîó –ò—Å–ø—Ä–∞–≤–ª–µ–Ω URL: {image_url[:60]}...")
                
                logger.info(f"  üì§ –ü—ã—Ç–∞–µ–º—Å—è –æ—Ç–ø—Ä–∞–≤–∏—Ç—å —Å –∫–∞—Ä—Ç–∏–Ω–∫–æ–π...")
                
                img_response = requests.get(image_url, timeout=10, headers={
                    'User-Agent': 'Mozilla/5.0'
                })
                
                if img_response.status_code == 200:
                    photo_data = {
                        'chat_id': CHANNEL_ID,
                        'caption': message_text,
                        'parse_mode': 'HTML'
                    }
                    
                    content_type = img_response.headers.get('Content-Type', 'image/jpeg')
                    
                    files = {'photo': ('image.jpg', img_response.content, content_type)}
                    
                    response = requests.post(
                        f'https://api.telegram.org/bot{BOT_TOKEN}/sendPhoto',
                        files=files,
                        data=photo_data,
                        timeout=20
                    )
                    
                    if response.status_code == 200:
                        logger.info("  ‚úÖ –°–æ–æ–±—â–µ–Ω–∏–µ —Å –∫–∞—Ä—Ç–∏–Ω–∫–æ–π –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ")
                        time.sleep(random.uniform(15, 25))
                        return True
                    else:
                        logger.warning(f"  ‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–ø—Ä–∞–≤–∏—Ç—å —Ñ–æ—Ç–æ: {response.status_code}")
                
            except Exception as e:
                logger.warning(f"  ‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å –∫–∞—Ä—Ç–∏–Ω–∫–æ–π: {str(e)[:80]}")
        
        logger.info("  üì§ –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Ç–µ–∫—Å—Ç–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ")
        data_text = {
            'chat_id': CHANNEL_ID,
            'text': message_text,
            'parse_mode': 'HTML',
            'disable_web_page_preview': 'true'
        }
        
        response = requests.post(
            f'https://api.telegram.org/bot{BOT_TOKEN}/sendMessage',
            data=data_text,
            timeout=10
        )
        
        if response.status_code == 200:
            logger.info("  ‚úÖ –¢–µ–∫—Å—Ç–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ")
            time.sleep(random.uniform(15, 25))
            return True
        else:
            logger.error(f"  ‚ùå –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏: {response.status_code}")
            return False
        
    except Exception as e:
        logger.error(f"ü§ñ –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        return False

def load_rss_feeds():
    """üì∞ –ó–∞–≥—Ä—É–∂–∞–µ—Ç RSS-–ª–µ–Ω—Ç—ã –∏ —Ö—ç—à—Ç–µ–≥–∏"""
    feeds = []
    hashtags = {}
    
    try:
        with open('feeds.txt', 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if not line or line.startswith('#'):
                    continue
                
                if '#' in line:
                    url, tag = line.split('#', 1)
                    feeds.append(url.strip())
                    hashtags[url.strip()] = '#' + tag.strip()
                else:
                    feeds.append(line)
                    hashtags[line] = '#–Ω–æ–≤–æ—Å—Ç–∏'
    
    except FileNotFoundError:
        logger.error("‚ùå –§–∞–π–ª feeds.txt –Ω–µ –Ω–∞–π–¥–µ–Ω")
        exit(1)
    
    if not feeds:
        logger.error("‚ùå –ù–µ—Ç RSS-–ª–µ–Ω—Ç")
        exit(1)
    
    logger.info(f"üì∞ –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {len(feeds)} –ª–µ–Ω—Ç")
    return feeds, hashtags

def load_dates():
    """üìÅ –ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—é –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã—Ö –ø–æ—Å—Ç–æ–≤"""
    try:
        with open('dates.json', 'r', encoding='utf-8') as f:
            data = json.load(f)
            for url, info in data.items():
                if 'last_date' in info:
                    info['last_date'] = datetime.fromisoformat(info['last_date'])
            return data
    except FileNotFoundError:
        return {}

def save_dates(dates_dict):
    """üíæ –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏—Å—Ç–æ—Ä–∏—é –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã—Ö –ø–æ—Å—Ç–æ–≤"""
    data_to_save = {}
    for url, info in dates_dict.items():
        if isinstance(info, dict) and 'last_date' in info:
            data_to_save[url] = {'last_date': info['last_date'].isoformat()}
    
    with open('dates.json', 'w', encoding='utf-8') as f:
        json.dump(data_to_save, f, indent=2, ensure_ascii=False)

def parse_feed(url):
    """üì∞ –ü–∞—Ä—Å–∏—Ç RSS-–ª–µ–Ω—Ç—É"""
    try:
        headers = {'User-Agent': 'Mozilla/5.0', 'Accept': 'application/rss+xml'}
        response = requests.get(url, headers=headers, timeout=10)
        feed = feedparser.parse(response.content)
        return feed if hasattr(feed, 'entries') and feed.entries else None
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ {url[:40]}...: {e}")
        return None

def get_entry_date(entry):
    """üìÖ –ü–æ–ª—É—á–∞–µ—Ç –¥–∞—Ç—É –∑–∞–ø–∏—Å–∏"""
    if hasattr(entry, 'published_parsed') and entry.published_parsed:
        return datetime(*entry.published_parsed[:6], tzinfo=timezone.utc)
    return datetime.now(timezone.utc)

# ==================== –û—Å–Ω–æ–≤–Ω–∞—è –ª–æ–≥–∏–∫–∞ ====================

def check_feeds():
    """üîç –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –ª–µ–Ω—Ç"""
    logger.info("=" * 60)
    logger.info(f"ü§ñ [{len(RSS_FEEDS)} –ª–µ–Ω—Ç] {datetime.now().strftime('%H:%M')}")
    start_time = time.time()

    dates = load_dates()
    sent_count = 0

    for feed_url in RSS_FEEDS:
        try:
            logger.info(f"üì∞ –ü—Ä–æ–≤–µ—Ä–∫–∞: {feed_url[:50]}...")
            
            last_date = dates.get(feed_url, {}).get('last_date')
            threshold_date = (datetime.now(timezone.utc) - timedelta(hours=CONFIG['MAX_HOURS_BACK']) 
                            if last_date is None else last_date)

            feed = parse_feed(feed_url)
            if not feed:
                time.sleep(random.uniform(CONFIG['REQUEST_DELAY_MIN'], CONFIG['REQUEST_DELAY_MAX']))
                continue

            new_entries = []
            for entry in feed.entries:
                entry_date = get_entry_date(entry)
                if entry_date > threshold_date:
                    new_entries.append((entry, entry_date))

            if new_entries:
                logger.info(f"  üì¶ –ù–∞–π–¥–µ–Ω–æ –Ω–æ–≤—ã—Ö: {len(new_entries)}")
                new_entries.sort(key=lambda x: x[1])

                for entry, pub_date in new_entries:
                    title = getattr(entry, 'title', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')
                    link = getattr(entry, 'link', '')
                    
                    if not link:
                        continue

                    logger.info(f"  üì§ –û—Ç–ø—Ä–∞–≤–∫–∞ [{pub_date.strftime('%H:%M')}]: {title[:60]}...")
                    
                    if send_to_telegram(title, link, feed_url, HASHTAGS, entry, pub_date):
                        sent_count += 1
                        dates[feed_url] = {'last_date': pub_date}
                        save_dates(dates)
                    else:
                        logger.error("  ‚ùå –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏")
                        break
            else:
                logger.info(f"  ‚úÖ –ù–µ—Ç –Ω–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π")

            time.sleep(random.uniform(CONFIG['REQUEST_DELAY_MIN'], CONFIG['REQUEST_DELAY_MAX']))

        except Exception as e:
            logger.error(f"  ‚ùå –û—à–∏–±–∫–∞: {e}")
            time.sleep(random.uniform(CONFIG['REQUEST_DELAY_MIN'], CONFIG['REQUEST_DELAY_MAX']))
            continue

    save_dates(dates)
    logger.info(f"üìä –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –û—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ: {sent_count} –Ω–æ–≤–æ—Å—Ç–µ–π")
    logger.info(f"‚è± –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {time.time() - start_time:.1f} —Å–µ–∫")
    logger.info("=" * 60)
    return sent_count

# ==================== –ó–∞–ø—É—Å–∫ ====================

if __name__ == '__main__':
    logger.info("=" * 60)
    RSS_FEEDS, HASHTAGS = load_rss_feeds()
    logger.info(f"‚è∞ –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏: {CONFIG['REQUEST_DELAY_MIN']}-{CONFIG['REQUEST_DELAY_MAX']} —Å–µ–∫")
    logger.info(f"‚è≥ –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–æ–≤–æ—Å—Ç–∏ –∑–∞: {CONFIG['MAX_HOURS_BACK']} —á–∞—Å–æ–≤")
    logger.info("=" * 60)
    
    sent_count = check_feeds()
    logger.info(f"‚úÖ –ë–æ—Ç –∑–∞–≤–µ—Ä—à–∏–ª —Ä–∞–±–æ—Ç—É. –û—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ: {sent_count} –ø–æ—Å—Ç–æ–≤")
