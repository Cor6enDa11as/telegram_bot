#!/usr/bin/env python3
"""
üöÄ RSS to Telegram Bot (GitHub Actions) - –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø
‚úÖ –§–ò–ö–° –¥—É–±–ª–µ–π: –ø–µ—Ä–≤—ã–π –∑–∞–ø—É—Å–∫=24—á, –æ—Å—Ç–∞–ª—å–Ω—ã–µ=—Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–µ –ø–æ—Å–ª–µ last_date
"""

import os
import json
import feedparser
import requests
import time
import logging
import random
import re
from datetime import datetime, timezone, timedelta
from urllib.parse import urlparse

# ==================== –ù–ê–°–¢–†–û–ô–ö–ò ====================
BOT_TOKEN = os.getenv('BOT_TOKEN')
CHANNEL_ID = os.getenv('CHANNEL_ID')

if not BOT_TOKEN or not CHANNEL_ID:
    print("‚ùå –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ BOT_TOKEN –∏ CHANNEL_ID –≤ GitHub Secrets!")
    exit(1)

CONFIG = {
    'REQUEST_DELAY_MIN': int(os.getenv('REQUEST_DELAY_MIN', '5')),
    'REQUEST_DELAY_MAX': int(os.getenv('REQUEST_DELAY_MAX', '10')),
    'MAX_HOURS_BACK': int(os.getenv('MAX_HOURS_BACK', '24'))  # –¢–æ–ª—å–∫–æ –¥–ª—è —Å–ø—Ä–∞–≤–∫–∏
}

RSS_FEEDS = []
HASHTAGS = {}

# ==================== –õ–û–ì–ò–†–û–í–ê–ù–ò–ï ====================
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ==================== –£–¢–ò–õ–ò–¢–´ ====================
def get_entry_image(entry):
    """üñºÔ∏è –ò—â–µ—Ç –∫–∞—Ä—Ç–∏–Ω–∫—É –≤ RSS: enclosures ‚Üí media ‚Üí thumbnail ‚Üí image"""
    candidates = [
        getattr(entry, 'enclosures', [{}])[0].get('href') if entry.enclosures else None,
        getattr(entry, 'media_content', [{}])[0].get('url') if hasattr(entry, 'media_content') and entry.media_content else None,
        getattr(entry, 'media_thumbnail', [{}])[0].get('url') if hasattr(entry, 'media_thumbnail') and entry.media_thumbnail else None,
        getattr(entry, 'image', {}).get('href') if hasattr(entry, 'image') else None,
    ]
    for img_url in candidates:
        if img_url and (img_url.startswith('http') or img_url.startswith('//')):
            if img_url.startswith('//'):  # –§–∏–∫—Å –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã—Ö URL
                base_url = getattr(entry, 'base', 'https://example.com')
                if base_url.startswith('http'):
                    parsed = urlparse(base_url)
                    img_url = f"{parsed.scheme}:{img_url}"
            return img_url
    return None

def clean_description(description):
    """üßπ –£–±–∏—Ä–∞–µ—Ç HTML, –æ–±—Ä–µ–∑–∞–µ—Ç –¥–æ 300 —Å–∏–º–≤–æ–ª–æ–≤"""
    if not description:
        return ''
    description = re.sub(r'<[^>]+>', '', description.strip())
    description = description.replace('&', '&amp;').replace('<', '&lt;').replace('>', '&gt;')
    return description[:300] + '...' if len(description) > 300 else description

def format_publication_date(pub_date):
    """üìÖ –§–æ—Ä–º–∞—Ç –¥–∞—Ç—ã: 25.12.2025 14:30"""
    return pub_date.strftime('%d.%m.%Y %H:%M')

# ==================== –û–¢–ü–†–ê–í–ö–ê –í TELEGRAM ====================
def send_to_telegram(title, link, feed_url, hashtags_dict, entry, pub_date):
    """üì§ –û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –ø–æ—Å—Ç —Å –∫–∞—Ä—Ç–∏–Ω–∫–æ–π (–µ—Å–ª–∏ –µ—Å—Ç—å) –∏–ª–∏ —Ç–µ–∫—Å—Ç–æ–º"""
    try:
        clean_title = title.replace('&', '&amp;').replace('<', '&lt;').replace('>', '&gt;')
        hashtag = hashtags_dict.get(feed_url, '#–Ω–æ–≤–æ—Å—Ç–∏')
        author = getattr(entry, 'author', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π').replace(" ", "")

        description = clean_description(getattr(entry, 'summary', '') or getattr(entry, 'description', ''))
        image_url = get_entry_image(entry)  # üîç –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –∫–∞—Ä—Ç–∏–Ω–∫–∏

        message_text = f'<a href="{link}">{clean_title}</a>'
        if description:
            message_text += f'\n\n<i>{description}</i>'
        message_text += f'\n\nüìå {hashtag} üë§ #{author}'

        # üé® –ü–†–ò–û–†–ò–¢–ï–¢ 1: –ö–∞—Ä—Ç–∏–Ω–∫–∞ (–µ—Å–ª–∏ –Ω–∞–π–¥–µ–Ω–∞)
        if image_url:
            try:
                if image_url.startswith('//'):
                    image_url = 'https:' + image_url

                img_response = requests.get(image_url, timeout=10, headers={'User-Agent': 'Mozilla/5.0'})
                if img_response.status_code == 200:
                    files = {'photo': ('image.jpg', img_response.content, img_response.headers.get('Content-Type', 'image/jpeg'))}
                    response = requests.post(
                        f'https://api.telegram.org/bot{BOT_TOKEN}/sendPhoto',
                        files=files,
                        data={'chat_id': CHANNEL_ID, 'caption': message_text, 'parse_mode': 'HTML'},
                        timeout=20
                    )
                    if response.status_code == 200:
                        logger.info("‚úÖ –ü–æ—Å—Ç —Å –∫–∞—Ä—Ç–∏–Ω–∫–æ–π –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω")
                        time.sleep(random.uniform(1, 3))
                        return True
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –ö–∞—Ä—Ç–∏–Ω–∫–∞ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∞: {str(e)[:50]}")

        # üìù –ü–†–ò–û–†–ò–¢–ï–¢ 2: –¢–µ–∫—Å—Ç –±–µ–∑ –ø—Ä–µ–≤—å—é
        data_text = {
            'chat_id': CHANNEL_ID,
            'text': message_text,
            'parse_mode': 'HTML',
            'disable_web_page_preview': 'true'
        }
        response = requests.post(f'https://api.telegram.org/bot{BOT_TOKEN}/sendMessage', data=data_text, timeout=10)

        if response.status_code == 200:
            logger.info("‚úÖ –¢–µ–∫—Å—Ç–æ–≤—ã–π –ø–æ—Å—Ç –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω")
            time.sleep(random.uniform(15, 25))
            return True
        else:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏: {response.status_code}")
            return False

    except Exception as e:
        logger.error(f"ü§ñ –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏: {e}")
        return False

# ==================== –†–ê–ë–û–¢–ê –° –§–ê–ô–õ–ê–ú–ò ====================
def load_rss_feeds():
    """üìÅ –ß–∏—Ç–∞–µ—Ç feeds.txt: URL#—Ö—ç—à—Ç–µ–≥ –∏–ª–∏ URL ‚Üí #–Ω–æ–≤–æ—Å—Ç–∏"""
    global RSS_FEEDS, HASHTAGS
    try:
        with open('feeds.txt', 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if not line or line.startswith('#'):
                    continue
                if '#' in line:
                    url, tag = line.split('#', 1)
                    RSS_FEEDS.append(url.strip())
                    HASHTAGS[url.strip()] = '#' + tag.strip()
                else:
                    RSS_FEEDS.append(line)
                    HASHTAGS[line] = '#–Ω–æ–≤–æ—Å—Ç–∏'
    except FileNotFoundError:
        logger.error("‚ùå feeds.txt –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        exit(1)

    if not RSS_FEEDS:
        logger.error("‚ùå –ù–µ—Ç RSS-–ª–µ–Ω—Ç!")
        exit(1)

    logger.info(f"üì∞ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(RSS_FEEDS)} –ª–µ–Ω—Ç")
    return RSS_FEEDS, HASHTAGS

def load_dates():
    """üìÖ –ß–∏—Ç–∞–µ—Ç dates.json, –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç —Å—Ç—Ä–æ–∫–∏ –≤ datetime"""
    try:
        with open('dates.json', 'r', encoding='utf-8') as f:
            data = json.load(f)
            for url, info in data.items():
                if 'last_date' in info:
                    data[url]['last_date'] = datetime.fromisoformat(info['last_date'])
            return data
    except FileNotFoundError:
        return {}  # ‚úÖ –ü–ï–†–í–´–ô –ó–ê–ü–£–°–ö

def save_dates(dates_dict):
    """üíæ –°–æ—Ö—Ä–∞–Ω—è–µ—Ç dates.json (—Ç–æ–ª—å–∫–æ last_date –∫–∞–∫ ISO —Å—Ç—Ä–æ–∫—É)"""
    data_to_save = {url: {'last_date': info['last_date'].isoformat()}
                   for url, info in dates_dict.items()
                   if isinstance(info, dict) and 'last_date' in info}
    with open('dates.json', 'w', encoding='utf-8') as f:
        json.dump(data_to_save, f, indent=2, ensure_ascii=False)

# ==================== RSS –ü–ê–†–°–ò–ù–ì ====================
def parse_feed(url):
    """üåê –°–∫–∞—á–∏–≤–∞–µ—Ç –∏ –ø–∞—Ä—Å–∏—Ç RSS"""
    try:
        headers = {'User-Agent': 'Mozilla/5.0', 'Accept': 'application/rss+xml'}
        response = requests.get(url, headers=headers, timeout=10)
        feed = feedparser.parse(response.content)
        return feed if hasattr(feed, 'entries') and feed.entries else None
    except Exception as e:
        logger.error(f"‚ùå –ü–∞—Ä—Å–∏–Ω–≥ {url[:40]}...: {e}")
        return None

def get_entry_date(entry):
    """üìÖ –ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–∞—Ç—É –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ (UTC)"""
    if hasattr(entry, 'published_parsed') and entry.published_parsed:
        return datetime(*entry.published_parsed[:6], tzinfo=timezone.utc)
    return datetime.now(timezone.utc)  # Fallback

# ==================== –û–°–ù–û–í–ù–ê–Ø –õ–û–ì–ò–ö–ê ‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–ê ====================
def check_feeds():
    """üîç –ì–õ–ê–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø - –ø—Ä–æ–≤–µ—Ä—è–µ—Ç –≤—Å–µ –ª–µ–Ω—Ç—ã"""
    logger.info("=" * 60)
    logger.info(f"ü§ñ [{len(RSS_FEEDS)} –ª–µ–Ω—Ç] {datetime.now().strftime('%H:%M')}")
    start_time = time.time()

    dates = load_dates()
    sent_count = 0

    for feed_url in RSS_FEEDS:
        logger.info(f"üì∞ {feed_url[:50]}...")

        # ‚úÖ –ö–õ–Æ–ß–ï–í–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï - –§–ò–ö–° –î–£–ë–õ–ï–ô!
        last_date = dates.get(feed_url, {}).get('last_date')
        if last_date is None:
            # üéØ –ü–ï–†–í–´–ô –ó–ê–ü–£–°–ö: —Ç–æ–ª—å–∫–æ —Å–≤–µ–∂–∏–µ –∑–∞ 24—á
            threshold_date = datetime.now(timezone.utc) - timedelta(hours=24)
            logger.info("  üîÑ –ü–ï–†–í–´–ô –∑–∞–ø—É—Å–∫: –∏—â–µ–º –∑–∞ 24—á")
        else:
            # üéØ –ü–û–°–õ–ï–î–£–Æ–©–ò–ï: —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–µ –ø–æ—Å–ª–µ last_date
            threshold_date = last_date
            logger.info(f"  ‚è∞ –° last_date: {last_date.strftime('%H:%M')}")

        feed = parse_feed(feed_url)
        if not feed:
            time.sleep(random.uniform(CONFIG['REQUEST_DELAY_MIN'], CONFIG['REQUEST_DELAY_MAX']))
            continue

        new_entries = []
        for entry in feed.entries:
            entry_date = get_entry_date(entry)
            if entry_date > threshold_date:  # ‚úÖ –¢–æ–ª—å–∫–æ –Ω–æ–≤—ã–µ!
                new_entries.append((entry, entry_date))

        if new_entries:
            logger.info(f"  üì¶ –ù–æ–≤—ã—Ö: {len(new_entries)}")
            new_entries.sort(key=lambda x: x[1])  # –ü–æ –¥–∞—Ç–µ (—Å—Ç–∞—Ä—ã–µ ‚Üí –Ω–æ–≤—ã–µ)

            for entry, pub_date in new_entries:
                title = getattr(entry, 'title', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')
                link = getattr(entry, 'link', '')
                if not link:
                    continue

                logger.info(f"  üì§ [{pub_date.strftime('%H:%M')}] {title[:60]}...")

                if send_to_telegram(title, link, feed_url, HASHTAGS, entry, pub_date):
                    sent_count += 1
                    dates[feed_url] = {'last_date': pub_date}  # ‚úÖ –û–±–Ω–æ–≤–ª—è–µ–º –¥–∞—Ç—É
                    save_dates(dates)  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ü–û–°–õ–ï –∫–∞–∂–¥–æ–π –æ—Ç–ø—Ä–∞–≤–∫–∏
                else:
                    logger.error("  ‚ùå –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏")
                    break
        else:
            logger.info("  ‚úÖ –ù–µ—Ç –Ω–æ–≤—ã—Ö")

        time.sleep(random.uniform(CONFIG['REQUEST_DELAY_MIN'], CONFIG['REQUEST_DELAY_MAX']))

    save_dates(dates)  # –§–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    logger.info(f"üìä –ó–∞–≤–µ—Ä—à–µ–Ω–æ. –û—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ: {sent_count}")
    logger.info(f"‚è±Ô∏è {time.time() - start_time:.1f} —Å–µ–∫")
    logger.info("=" * 60)
    return sent_count

# ==================== –ó–ê–ü–£–°–ö ====================
if __name__ == '__main__':
    logger.info("=" * 60)
    load_rss_feeds()
    logger.info(f"‚è∞ –ó–∞–¥–µ—Ä–∂–∫–∏: {CONFIG['REQUEST_DELAY_MIN']}-{CONFIG['REQUEST_DELAY_MAX']}—Å")
    logger.info(f"üÜï –õ–æ–≥–∏–∫–∞: –ø–µ—Ä–≤—ã–π –∑–∞–ø—É—Å–∫=24—á, –¥–∞–ª–µ–µ=—Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–µ")
    logger.info("=" * 60)

    sent_count = check_feeds()
    logger.info(f"‚úÖ –ì–û–¢–û–í–û! –û—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ: {sent_count} –ø–æ—Å—Ç–æ–≤ üöÄ")
